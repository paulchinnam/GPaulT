{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiXE4i1E7FoB0FYOnUVBHF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulchinnam/GPaulT/blob/main/GPaulT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXgHaasJVThI",
        "outputId": "6b843c5f-9deb-4d8e-de16-dc2cb5bffba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-23 16:14:14--  https://raw.githubusercontent.com/paulchinnam/GPaulT/main/TrainingData/MovieScripts/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95297 (93K) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]  93.06K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-03-23 16:14:14 (5.31 MB/s) - ‘input.txt’ saved [95297/95297]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Avengers Infinity War Script for training\n",
        "\n",
        "!wget https://raw.githubusercontent.com/paulchinnam/GPaulT/main/TrainingData/MovieScripts/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read script in\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "KPzVb4PaXET3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print length of dataset\n",
        "\n",
        "print(\"Length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYyXvxdoXbRT",
        "outputId": "a7e842d5-02a5-4e75-e649-0e5d45520418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  92772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first 1000 characters\n",
        "\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHwM2WNrXypw",
        "outputId": "0b2b706d-0809-41af-903f-e3794facfa0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[radio transmission sound]\n",
            "\n",
            "[Asgardian PA]: This is the Asgardian refugee vessel Statesman! We are under assault, I repeat, we are under assault – The engines are dead, life support failing! Requesting aid from any vessel within range... We are 22 jump points out of Asgard. Our crew is made up of Asgardian families, we have very few soldiers here! This is not a war craft, I repeat, this is not a war craft!\n",
            "\n",
            "[Inside the ship, Ebony Maw walks among the bodies of dead Asgardians. He steps over them as he speaks with no mind, as if they were scattered pieces of dirty clothing on a bedroom floor.]\n",
            "\n",
            "Ebony Maw: Hear me, and rejoice. You have had the privilege of being saved by the Great Titan. You may think this is suffering. No... It is salvation. Universal scales tip toward balance because of your sacrifice. Smile... For even in death, you have become Children of Thanos.\n",
            "\n",
            "[Loki is seen with the Black Order. He watches Thanos.]\n",
            "\n",
            "Thanos: [Looking out the large window.] I know what it's like t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all unique characters in the dataset\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdFq5QXRYvUO",
        "outputId": "a8e5d242-ed22-4e96-a5c5-8154756f9110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !%&'(),-.012345678:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz{}–‘“”\n",
            "84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping from characters to integers\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW2eErP9ZSk2",
        "outputId": "e0cf6d6f-bd24-49bb-cd6d-4d50dd63fc1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[59, 60, 60, 1, 71, 59, 56, 69, 56]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the entire dataset and store in a torch.Tensor\n",
        "\n",
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # This is what the previous 1000 characters will look like to the GPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4yJGcAGZ42j",
        "outputId": "656cd506-76cb-4ece-da3a-625970464590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([92772]) torch.int64\n",
            "tensor([49, 69, 52, 55, 60, 66,  1, 71, 69, 52, 65, 70, 64, 60, 70, 70, 60, 66,\n",
            "        65,  1, 70, 66, 72, 65, 55, 50,  0,  0, 49, 23, 70, 58, 52, 69, 55, 60,\n",
            "        52, 65,  1, 38, 23, 50, 20,  1, 42, 59, 60, 70,  1, 60, 70,  1, 71, 59,\n",
            "        56,  1, 23, 70, 58, 52, 69, 55, 60, 52, 65,  1, 69, 56, 57, 72, 58, 56,\n",
            "        56,  1, 73, 56, 70, 70, 56, 63,  1, 41, 71, 52, 71, 56, 70, 64, 52, 65,\n",
            "         2,  1, 45, 56,  1, 52, 69, 56,  1, 72, 65, 55, 56, 69,  1, 52, 70, 70,\n",
            "        52, 72, 63, 71,  8,  1, 31,  1, 69, 56, 67, 56, 52, 71,  8,  1, 74, 56,\n",
            "         1, 52, 69, 56,  1, 72, 65, 55, 56, 69,  1, 52, 70, 70, 52, 72, 63, 71,\n",
            "         1, 80,  1, 42, 59, 56,  1, 56, 65, 58, 60, 65, 56, 70,  1, 52, 69, 56,\n",
            "         1, 55, 56, 52, 55,  8,  1, 63, 60, 57, 56,  1, 70, 72, 67, 67, 66, 69,\n",
            "        71,  1, 57, 52, 60, 63, 60, 65, 58,  2,  1, 40, 56, 68, 72, 56, 70, 71,\n",
            "        60, 65, 58,  1, 52, 60, 55,  1, 57, 69, 66, 64,  1, 52, 65, 76,  1, 73,\n",
            "        56, 70, 70, 56, 63,  1, 74, 60, 71, 59, 60, 65,  1, 69, 52, 65, 58, 56,\n",
            "        10, 10, 10,  1, 45, 56,  1, 52, 69, 56,  1, 13, 13,  1, 61, 72, 64, 67,\n",
            "         1, 67, 66, 60, 65, 71, 70,  1, 66, 72, 71,  1, 66, 57,  1, 23, 70, 58,\n",
            "        52, 69, 55, 10,  1, 37, 72, 69,  1, 54, 69, 56, 74,  1, 60, 70,  1, 64,\n",
            "        52, 55, 56,  1, 72, 67,  1, 66, 57,  1, 23, 70, 58, 52, 69, 55, 60, 52,\n",
            "        65,  1, 57, 52, 64, 60, 63, 60, 56, 70,  8,  1, 74, 56,  1, 59, 52, 73,\n",
            "        56,  1, 73, 56, 69, 76,  1, 57, 56, 74,  1, 70, 66, 63, 55, 60, 56, 69,\n",
            "        70,  1, 59, 56, 69, 56,  2,  1, 42, 59, 60, 70,  1, 60, 70,  1, 65, 66,\n",
            "        71,  1, 52,  1, 74, 52, 69,  1, 54, 69, 52, 57, 71,  8,  1, 31,  1, 69,\n",
            "        56, 67, 56, 52, 71,  8,  1, 71, 59, 60, 70,  1, 60, 70,  1, 65, 66, 71,\n",
            "         1, 52,  1, 74, 52, 69,  1, 54, 69, 52, 57, 71,  2,  0,  0, 49, 31, 65,\n",
            "        70, 60, 55, 56,  1, 71, 59, 56,  1, 70, 59, 60, 67,  8,  1, 27, 53, 66,\n",
            "        65, 76,  1, 35, 52, 74,  1, 74, 52, 63, 62, 70,  1, 52, 64, 66, 65, 58,\n",
            "         1, 71, 59, 56,  1, 53, 66, 55, 60, 56, 70,  1, 66, 57,  1, 55, 56, 52,\n",
            "        55,  1, 23, 70, 58, 52, 69, 55, 60, 52, 65, 70, 10,  1, 30, 56,  1, 70,\n",
            "        71, 56, 67, 70,  1, 66, 73, 56, 69,  1, 71, 59, 56, 64,  1, 52, 70,  1,\n",
            "        59, 56,  1, 70, 67, 56, 52, 62, 70,  1, 74, 60, 71, 59,  1, 65, 66,  1,\n",
            "        64, 60, 65, 55,  8,  1, 52, 70,  1, 60, 57,  1, 71, 59, 56, 76,  1, 74,\n",
            "        56, 69, 56,  1, 70, 54, 52, 71, 71, 56, 69, 56, 55,  1, 67, 60, 56, 54,\n",
            "        56, 70,  1, 66, 57,  1, 55, 60, 69, 71, 76,  1, 54, 63, 66, 71, 59, 60,\n",
            "        65, 58,  1, 66, 65,  1, 52,  1, 53, 56, 55, 69, 66, 66, 64,  1, 57, 63,\n",
            "        66, 66, 69, 10, 50,  0,  0, 27, 53, 66, 65, 76,  1, 35, 52, 74, 20,  1,\n",
            "        30, 56, 52, 69,  1, 64, 56,  8,  1, 52, 65, 55,  1, 69, 56, 61, 66, 60,\n",
            "        54, 56, 10,  1, 47, 66, 72,  1, 59, 52, 73, 56,  1, 59, 52, 55,  1, 71,\n",
            "        59, 56,  1, 67, 69, 60, 73, 60, 63, 56, 58, 56,  1, 66, 57,  1, 53, 56,\n",
            "        60, 65, 58,  1, 70, 52, 73, 56, 55,  1, 53, 76,  1, 71, 59, 56,  1, 29,\n",
            "        69, 56, 52, 71,  1, 42, 60, 71, 52, 65, 10,  1, 47, 66, 72,  1, 64, 52,\n",
            "        76,  1, 71, 59, 60, 65, 62,  1, 71, 59, 60, 70,  1, 60, 70,  1, 70, 72,\n",
            "        57, 57, 56, 69, 60, 65, 58, 10,  1, 36, 66, 10, 10, 10,  1, 31, 71,  1,\n",
            "        60, 70,  1, 70, 52, 63, 73, 52, 71, 60, 66, 65, 10,  1, 43, 65, 60, 73,\n",
            "        56, 69, 70, 52, 63,  1, 70, 54, 52, 63, 56, 70,  1, 71, 60, 67,  1, 71,\n",
            "        66, 74, 52, 69, 55,  1, 53, 52, 63, 52, 65, 54, 56,  1, 53, 56, 54, 52,\n",
            "        72, 70, 56,  1, 66, 57,  1, 76, 66, 72, 69,  1, 70, 52, 54, 69, 60, 57,\n",
            "        60, 54, 56, 10,  1, 41, 64, 60, 63, 56, 10, 10, 10,  1, 28, 66, 69,  1,\n",
            "        56, 73, 56, 65,  1, 60, 65,  1, 55, 56, 52, 71, 59,  8,  1, 76, 66, 72,\n",
            "         1, 59, 52, 73, 56,  1, 53, 56, 54, 66, 64, 56,  1, 25, 59, 60, 63, 55,\n",
            "        69, 56, 65,  1, 66, 57,  1, 42, 59, 52, 65, 66, 70, 10,  0,  0, 49, 34,\n",
            "        66, 62, 60,  1, 60, 70,  1, 70, 56, 56, 65,  1, 74, 60, 71, 59,  1, 71,\n",
            "        59, 56,  1, 24, 63, 52, 54, 62,  1, 37, 69, 55, 56, 69, 10,  1, 30, 56,\n",
            "         1, 74, 52, 71, 54, 59, 56, 70,  1, 42, 59, 52, 65, 66, 70, 10, 50,  0,\n",
            "         0, 42, 59, 52, 65, 66, 70, 20,  1, 49, 34, 66, 66, 62, 60, 65, 58,  1,\n",
            "        66, 72, 71,  1, 71, 59, 56,  1, 63, 52, 69, 58, 56,  1, 74, 60, 65, 55,\n",
            "        66, 74, 10, 50,  1, 31,  1, 62, 65, 66, 74,  1, 74, 59, 52, 71,  1, 60,\n",
            "        71,  5, 70,  1, 63, 60, 62, 56,  1, 71])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets (90/10 ratio)\n",
        "\n",
        "n = int(0.9*len(data)) # Calculate 90% of the length of the data\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "ZlrRAHLwc_Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE\n",
        "\n",
        "block_size = 8\n",
        "train_data[:block_size + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEJ7u2gGdtqE",
        "outputId": "6f4ab186-cc42-4aeb-e812-c67742a8eaa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([49, 69, 52, 55, 60, 66,  1, 71, 69])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size + 1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t + 1]\n",
        "  target = y[t]\n",
        "  print(f\"When input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfLd9hm5gHZH",
        "outputId": "493603ca-ae14-4ff7-fbfd-4a642871c7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([49]) the target: 69\n",
            "When input is tensor([49, 69]) the target: 52\n",
            "When input is tensor([49, 69, 52]) the target: 55\n",
            "When input is tensor([49, 69, 52, 55]) the target: 60\n",
            "When input is tensor([49, 69, 52, 55, 60]) the target: 66\n",
            "When input is tensor([49, 69, 52, 55, 60, 66]) the target: 1\n",
            "When input is tensor([49, 69, 52, 55, 60, 66,  1]) the target: 71\n",
            "When input is tensor([49, 69, 52, 55, 60, 66,  1, 71]) the target: 69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly sample the data and build 4x8 tensors\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  # Generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "  y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
        "\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('Inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('Targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t + 1]\n",
        "    target = yb[b,t]\n",
        "\n",
        "    print(f\"When input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptYoGjhdhmYw",
        "outputId": "2e4751de-7e59-4baa-82bc-256308f7bd0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 0, 42, 59, 52, 65, 66, 70, 20],\n",
            "        [ 1, 52,  1, 63, 66, 71, 10,  1],\n",
            "        [ 1, 66, 72, 71, 10,  1, 41, 71],\n",
            "        [58,  1, 52,  1, 65, 56, 74,  1]])\n",
            "Targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[42, 59, 52, 65, 66, 70, 20,  1],\n",
            "        [52,  1, 63, 66, 71, 10,  1,  6],\n",
            "        [66, 72, 71, 10,  1, 41, 71, 69],\n",
            "        [ 1, 52,  1, 65, 56, 74,  1, 59]])\n",
            "----\n",
            "When input is [0] the target: 42\n",
            "When input is [0, 42] the target: 59\n",
            "When input is [0, 42, 59] the target: 52\n",
            "When input is [0, 42, 59, 52] the target: 65\n",
            "When input is [0, 42, 59, 52, 65] the target: 66\n",
            "When input is [0, 42, 59, 52, 65, 66] the target: 70\n",
            "When input is [0, 42, 59, 52, 65, 66, 70] the target: 20\n",
            "When input is [0, 42, 59, 52, 65, 66, 70, 20] the target: 1\n",
            "When input is [1] the target: 52\n",
            "When input is [1, 52] the target: 1\n",
            "When input is [1, 52, 1] the target: 63\n",
            "When input is [1, 52, 1, 63] the target: 66\n",
            "When input is [1, 52, 1, 63, 66] the target: 71\n",
            "When input is [1, 52, 1, 63, 66, 71] the target: 10\n",
            "When input is [1, 52, 1, 63, 66, 71, 10] the target: 1\n",
            "When input is [1, 52, 1, 63, 66, 71, 10, 1] the target: 6\n",
            "When input is [1] the target: 66\n",
            "When input is [1, 66] the target: 72\n",
            "When input is [1, 66, 72] the target: 71\n",
            "When input is [1, 66, 72, 71] the target: 10\n",
            "When input is [1, 66, 72, 71, 10] the target: 1\n",
            "When input is [1, 66, 72, 71, 10, 1] the target: 41\n",
            "When input is [1, 66, 72, 71, 10, 1, 41] the target: 71\n",
            "When input is [1, 66, 72, 71, 10, 1, 41, 71] the target: 69\n",
            "When input is [58] the target: 1\n",
            "When input is [58, 1] the target: 52\n",
            "When input is [58, 1, 52] the target: 1\n",
            "When input is [58, 1, 52, 1] the target: 65\n",
            "When input is [58, 1, 52, 1, 65] the target: 56\n",
            "When input is [58, 1, 52, 1, 65, 56] the target: 74\n",
            "When input is [58, 1, 52, 1, 65, 56, 74] the target: 1\n",
            "When input is [58, 1, 52, 1, 65, 56, 74, 1] the target: 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE\n",
        "\n",
        "print(xb) # Transformer input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD94WWP83BzW",
        "outputId": "c293580a-a2b9-4799-8bf4-c819efa90abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0, 42, 59, 52, 65, 66, 70, 20],\n",
            "        [ 1, 52,  1, 63, 66, 71, 10,  1],\n",
            "        [ 1, 66, 72, 71, 10,  1, 41, 71],\n",
            "        [58,  1, 52,  1, 65, 56, 74,  1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    # idx and target are both (B, T) tensor of integers\n",
        "    logits = self.token_embedding_table(idx) # (B (batch), T (time), C (channel (vocab size)))\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is array of (B, T) indices in current context\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      # Get the predictions\n",
        "      logits, loss = self(idx)\n",
        "\n",
        "      # Focus on last time step\n",
        "      logits = logits[:, -1, :] # Becomes (B, C)\n",
        "\n",
        "      # Apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "      # Sample from distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "      # Append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "ZXf-Urqeh3xu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "263597c9-5859-460b-d6d6-1a41cff31182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 84])\n",
            "tensor(5.2030, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "&D(&'Fw7?hLH&g\n",
            "TKKNhbyPPyM\n",
            "H6QD_2UEQN_w1GX)_%n‘TcHUVzshjG!oSza4oMi?M'jHPy_VL3!57UXbNn‘q_5dd:RY7YDHeX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pytorch optimizer\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "Vv0MWsXi0390"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "for steps in range(10000):\n",
        "\n",
        "  # Get sample batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # Evaluate loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xJMoBhy2uTO",
        "outputId": "a54990aa-7eb8-4cd9-a3c5-9873ec9652d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.36946702003479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bnoMBPa2y-n",
        "outputId": "095a9ef5-9a3c-4fe1-da20-5763cbde9dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "E VItld hictes QzXI opid Strotallinghullandso tha gr toulo d spe ane?”M{Gave led. g we bannf Nouny bee Po Dous whably fring hof k. w'suasha Yong n The..\n",
            "\n",
            "EThin, ayee, ck, rosto Ist sn abes, ketou's cka therem s llefou ouikn tan t'can ho rd: roo, ron, pththa6Be os. haitrd y. o wanse twiatests ct ichat'stouly-seruse d. ccu, plears sen baran.\n",
            "Romatalck “As t, then his l m.)\n",
            "\n",
            "\n",
            "Gr Kistimek: hand th): s find, y d) fe..\n",
            "Dreal Aw MIDisthat f tanearitrer I y 1Yooulck I rer, pole Brt St Yonony thicoit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.rand(B, T, C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtUp51EzDMQ0",
        "outputId": "d4854728-fd23-4971-d1a7-6f09fb709a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B, T, C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t + 1]\n",
        "    xbow[b, t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "oDXAFqRcDsgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ---> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niCGztscGUeh",
        "outputId": "1bb98efa-fb01-41d2-da52-02d5d39a4b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 3: Use Softmax\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.ones((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo1-MAqJHi6P",
        "outputId": "06e90311-5314-4866-edc6-671f9f1c1a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 4: Self-Attention\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "# wei = torch.zeros(T, T)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "# out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcm6XKFHNsbV",
        "outputId": "7aa8b429-bdb4-4eff-8211-f534303b4fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}